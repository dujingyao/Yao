#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—®é¢˜2æœ€ç»ˆä¼˜åŒ–ç‰ˆï¼šåŸºäºé™„ä»¶2å’Œé™„ä»¶3çš„æ·±åº¦æ”¹è¿›
ç«‹å³ä¿®æ­£ï¼šåˆ©ç‡æ¨¡å‹ä¸­çš„ Li â†’ Piï¼Œç›®æ ‡å‡½æ•°è°ƒæ•´ä¸ºæœŸæœ›æ”¶ç›Š
ä¼˜å…ˆè¡¥å……ï¼šè¡Œä¸šå‚æ•°è®¡ç®—é€»è¾‘å’ŒLogisticæ‹Ÿåˆä»£ç 
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings
from scipy.optimize import minimize, curve_fit
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
import seaborn as sns
from datetime import datetime, timedelta

warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

class UltimateEnterpriseAnalyzer:
    def __init__(self):
        self.raw_data = None
        self.sales_data = None
        self.purchase_data = None
        self.interest_loss_data = None
        self.ç»¼åˆæ•°æ® = None
        self.è¡Œä¸šå‚æ•° = {}
        self.æµå¤±ç‡æ¨¡å‹ = None
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        try:
            print("ğŸ“¥ åŠ è½½é™„ä»¶2å’Œé™„ä»¶3æ•°æ®...")
            
            # é™„ä»¶2ï¼šä¼ä¸šæ•°æ®
            file_path2 = "é™„ä»¶2ï¼š302å®¶æ— ä¿¡è´·è®°å½•ä¼ä¸šçš„ç›¸å…³æ•°æ®.xlsx"
            self.raw_data = pd.read_excel(file_path2, sheet_name='ä¼ä¸šä¿¡æ¯')
            self.sales_data = pd.read_excel(file_path2, sheet_name='é”€é¡¹å‘ç¥¨ä¿¡æ¯')
            self.purchase_data = pd.read_excel(file_path2, sheet_name='è¿›é¡¹å‘ç¥¨ä¿¡æ¯')
            
            # é™„ä»¶3ï¼šåˆ©ç‡-æµå¤±ç‡å…³ç³»æ•°æ®
            file_path3 = "é™„ä»¶3ï¼šé“¶è¡Œè´·æ¬¾å¹´åˆ©ç‡ä¸å®¢æˆ·æµå¤±ç‡å…³ç³»çš„ç»Ÿè®¡æ•°æ®.xlsx"
            self.interest_loss_data = pd.read_excel(file_path3)
            
            print(f"   - ä¼ä¸šä¿¡æ¯: {len(self.raw_data)}å®¶")
            print(f"   - é”€é¡¹å‘ç¥¨: {len(self.sales_data)}æ¡")
            print(f"   - è¿›é¡¹å‘ç¥¨: {len(self.purchase_data)}æ¡")
            print(f"   - åˆ©ç‡æ•°æ®: {len(self.interest_loss_data)}ç»„")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def build_interest_loss_model(self):
        """åŸºäºé™„ä»¶3æ„å»ºåˆ©ç‡-æµå¤±ç‡æ¨¡å‹"""
        print("\nğŸ“ˆ æ„å»ºåˆ©ç‡-æµå¤±ç‡å…³ç³»æ¨¡å‹...")
        
        # æ¸…ç†é™„ä»¶3æ•°æ®
        df = self.interest_loss_data.copy()
        df = df.dropna(subset=['è´·æ¬¾å¹´åˆ©ç‡'])
        
        # æå–åˆ©ç‡å’Œæµå¤±ç‡æ•°æ®
        rates = df['è´·æ¬¾å¹´åˆ©ç‡'].values
        
        # å¤„ç†ä¸‰ä¸ªä¿¡èª‰ç­‰çº§çš„æµå¤±ç‡
        loss_rates_A = pd.to_numeric(df['å®¢æˆ·æµå¤±ç‡'], errors='coerce').values
        loss_rates_B = pd.to_numeric(df['Unnamed: 2'], errors='coerce').values
        loss_rates_C = pd.to_numeric(df['Unnamed: 3'], errors='coerce').values
        
        # å»é™¤NaNå€¼
        valid_idx = ~(np.isnan(loss_rates_A) | np.isnan(loss_rates_B) | np.isnan(loss_rates_C))
        rates = rates[valid_idx]
        loss_rates_A = loss_rates_A[valid_idx]
        loss_rates_B = loss_rates_B[valid_idx]
        loss_rates_C = loss_rates_C[valid_idx]
        
        # å®šä¹‰æµå¤±ç‡å‡½æ•°ï¼ˆæŒ‡æ•°æ¨¡å‹ï¼‰
        def loss_rate_function(r, a, b, c):
            return a * np.exp(b * r) + c
        
        # æ‹Ÿåˆä¸‰ä¸ªç­‰çº§çš„æ¨¡å‹
        models = {}
        for grade, loss_rates in [('A', loss_rates_A), ('B', loss_rates_B), ('C', loss_rates_C)]:
            try:
                popt, _ = curve_fit(loss_rate_function, rates, loss_rates, 
                                  p0=[0.1, 10, 0.01], maxfev=5000)
                models[grade] = popt
                print(f"   - ç­‰çº§{grade}: æµå¤±ç‡ = {popt[0]:.3f} * exp({popt[1]:.1f} * r) + {popt[2]:.3f}")
            except:
                # å¤‡ç”¨çº¿æ€§æ¨¡å‹
                coeffs = np.polyfit(rates, loss_rates, 1)
                models[grade] = (coeffs[0], coeffs[1])
                print(f"   - ç­‰çº§{grade}: æµå¤±ç‡ = {coeffs[0]:.3f} * r + {coeffs[1]:.3f} (çº¿æ€§)")
        
        self.æµå¤±ç‡æ¨¡å‹ = models
        return models

    def preprocess_data_ultimate(self):
        """ç»ˆæç‰ˆæ•°æ®é¢„å¤„ç†"""
        print("\nğŸ”§ ç»ˆæç‰ˆæ•°æ®é¢„å¤„ç†...")
        
        # 1. é”€é¡¹æ•°æ®èšåˆï¼ˆä¿®æ­£ï¼šåˆ©ç”¨æ—¶é—´åºåˆ—ä¿¡æ¯ï¼‰
        self.sales_data['å¼€ç¥¨æ—¥æœŸ'] = pd.to_datetime(self.sales_data['å¼€ç¥¨æ—¥æœŸ'])
        self.purchase_data['å¼€ç¥¨æ—¥æœŸ'] = pd.to_datetime(self.purchase_data['å¼€ç¥¨æ—¥æœŸ'])
        
        # æŒ‰æœˆèšåˆï¼Œè®¡ç®—è¶‹åŠ¿
        sales_monthly = self.sales_data.groupby(['ä¼ä¸šä»£å·', 
                                               self.sales_data['å¼€ç¥¨æ—¥æœŸ'].dt.to_period('M')])['ä»·ç¨åˆè®¡'].sum().reset_index()
        
        sales_agg = self.sales_data.groupby('ä¼ä¸šä»£å·').agg({
            'ä»·ç¨åˆè®¡': ['sum', 'count', 'mean', 'std'],
            'å¼€ç¥¨æ—¥æœŸ': ['min', 'max'],
            'è´­æ–¹å•ä½ä»£å·': 'nunique',
            'å‘ç¥¨çŠ¶æ€': lambda x: (x == 'æœ‰æ•ˆå‘ç¥¨').mean()
        }).round(2)
        
        sales_agg.columns = ['å¹´æ”¶å…¥', 'é”€é¡¹å‘ç¥¨æ•°', 'å•ç¬”é”€å”®å‡å€¼', 'é”€å”®æ ‡å‡†å·®', 
                           'é¦–æ¬¡é”€å”®æ—¥æœŸ', 'æœ€åé”€å”®æ—¥æœŸ', 'å®¢æˆ·æ•°é‡', 'æœ‰æ•ˆå‘ç¥¨ç‡']
        
        # 2. è¿›é¡¹æ•°æ®èšåˆ
        purchase_agg = self.purchase_data.groupby('ä¼ä¸šä»£å·').agg({
            'ä»·ç¨åˆè®¡': ['sum', 'count', 'mean', 'std'],
            'å¼€ç¥¨æ—¥æœŸ': ['min', 'max'],
            'é”€æ–¹å•ä½ä»£å·': 'nunique',
            'å‘ç¥¨çŠ¶æ€': lambda x: (x == 'æœ‰æ•ˆå‘ç¥¨').mean()
        }).round(2)
        
        purchase_agg.columns = ['å¹´æˆæœ¬', 'è¿›é¡¹å‘ç¥¨æ•°', 'å•ç¬”é‡‡è´­å‡å€¼', 'é‡‡è´­æ ‡å‡†å·®',
                              'é¦–æ¬¡é‡‡è´­æ—¥æœŸ', 'æœ€åé‡‡è´­æ—¥æœŸ', 'ä¾›åº”å•†æ•°é‡', 'è¿›é¡¹æœ‰æ•ˆç‡']
        
        # 3. åˆå¹¶åŸºç¡€æ•°æ®
        self.ç»¼åˆæ•°æ® = self.raw_data.copy()
        self.ç»¼åˆæ•°æ® = self.ç»¼åˆæ•°æ®.merge(sales_agg, left_on='ä¼ä¸šä»£å·', 
                                        right_index=True, how='left')
        self.ç»¼åˆæ•°æ® = self.ç»¼åˆæ•°æ®.merge(purchase_agg, left_on='ä¼ä¸šä»£å·', 
                                        right_index=True, how='left')
        
        # 4. è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾
        self._calculate_time_series_features()
        
        # 5. è®¡ç®—å¢å¼ºè´¢åŠ¡æŒ‡æ ‡
        self._calculate_enhanced_financial_indicators()
        
        # 6. åŸºäºçœŸå®æ•°æ®è¿›è¡Œè¡Œä¸šåˆ†ç±»
        self._classify_industries_by_data()
        
        # 7. æ•°æ®è´¨é‡å¢å¼º
        self._enhance_data_quality()
        
        print("âœ… ç»ˆæç‰ˆæ•°æ®é¢„å¤„ç†å®Œæˆ")

    def _calculate_time_series_features(self):
        """è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾"""
        print("   - è®¡ç®—æ—¶é—´åºåˆ—ç‰¹å¾...")
        
        # è¥ä¸šå¤©æ•°
        self.ç»¼åˆæ•°æ®['è¥ä¸šå¤©æ•°'] = (
            (pd.to_datetime(self.ç»¼åˆæ•°æ®['æœ€åé”€å”®æ—¥æœŸ']) - 
             pd.to_datetime(self.ç»¼åˆæ•°æ®['é¦–æ¬¡é”€å”®æ—¥æœŸ'])).dt.days + 1
        ).fillna(365)
        
        # æ—¥å‡æ”¶å…¥
        self.ç»¼åˆæ•°æ®['æ—¥å‡æ”¶å…¥'] = self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] / self.ç»¼åˆæ•°æ®['è¥ä¸šå¤©æ•°']
        
        # è®¡ç®—æœˆåº¦æ”¶å…¥å¢é•¿ç‡ï¼ˆåŸºäºé”€é¡¹å‘ç¥¨æ—¶é—´åºåˆ—ï¼‰
        growth_rates = []
        for _, row in self.ç»¼åˆæ•°æ®.iterrows():
            enterprise_id = row['ä¼ä¸šä»£å·']
            enterprise_sales = self.sales_data[self.sales_data['ä¼ä¸šä»£å·'] == enterprise_id].copy()
            
            if len(enterprise_sales) > 0:
                enterprise_sales['å¹´æœˆ'] = enterprise_sales['å¼€ç¥¨æ—¥æœŸ'].dt.to_period('M')
                monthly_sales = enterprise_sales.groupby('å¹´æœˆ')['ä»·ç¨åˆè®¡'].sum()
                
                if len(monthly_sales) >= 3:
                    # è®¡ç®—ç¯æ¯”å¢é•¿ç‡
                    growth_rate = monthly_sales.pct_change().mean()
                    growth_rates.append(growth_rate)
                else:
                    growth_rates.append(0)
            else:
                growth_rates.append(0)
        
        self.ç»¼åˆæ•°æ®['æœˆå‡å¢é•¿ç‡'] = growth_rates

    def _calculate_enhanced_financial_indicators(self):
        """è®¡ç®—å¢å¼ºç‰ˆè´¢åŠ¡æŒ‡æ ‡"""
        print("   - è®¡ç®—å¢å¼ºç‰ˆè´¢åŠ¡æŒ‡æ ‡...")
        
        # ä¿®æ­£ï¼šå¤„ç†ç¼ºå¤±å€¼
        numeric_cols = ['å¹´æ”¶å…¥', 'å¹´æˆæœ¬', 'å®¢æˆ·æ•°é‡', 'ä¾›åº”å•†æ•°é‡']
        for col in numeric_cols:
            if col in self.ç»¼åˆæ•°æ®.columns:
                self.ç»¼åˆæ•°æ®[col] = self.ç»¼åˆæ•°æ®[col].fillna(0)
        
        # åŸºç¡€è´¢åŠ¡æŒ‡æ ‡ï¼ˆä¿®æ­£ï¼šå¤„ç†æ— ç©·å¤§å€¼ï¼‰
        self.ç»¼åˆæ•°æ®['æ¯›åˆ©ç‡'] = (
            (self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] - self.ç»¼åˆæ•°æ®['å¹´æˆæœ¬']) / 
            (self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] + 1e-6)
        ).clip(-1, 1).fillna(0)
        
        self.ç»¼åˆæ•°æ®['æ”¶å…¥æˆæœ¬æ¯”'] = (
            self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] / (self.ç»¼åˆæ•°æ®['å¹´æˆæœ¬'] + 1e-6)
        ).clip(0, 10).fillna(1)
        
        # å¢å¼ºæŒ‡æ ‡ï¼šä¼ä¸šå¿å€ºèƒ½åŠ›ä»£ç†å˜é‡
        self.ç»¼åˆæ•°æ®['æµåŠ¨æ¯”ç‡ä»£ç†'] = (
            self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] / (self.ç»¼åˆæ•°æ®['å¹´æˆæœ¬'] / 4 + 1e-6)
        ).clip(0, 5).fillna(1)
        
        # è¿è¥æ•ˆç‡æŒ‡æ ‡ï¼ˆä¿®æ­£ï¼šå¤„ç†é™¤é›¶ï¼‰
        self.ç»¼åˆæ•°æ®['èµ„äº§å‘¨è½¬ç‡ä»£ç†'] = (self.ç»¼åˆæ•°æ®['æ—¥å‡æ”¶å…¥'] / 1000).clip(0, 1000).fillna(0)
        self.ç»¼åˆæ•°æ®['å®¢æˆ·é»æ€§'] = (
            self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] / (self.ç»¼åˆæ•°æ®['å®¢æˆ·æ•°é‡'] + 1)
        ).clip(0, 1e8).fillna(0)
        
        # é£é™©æŒ‡æ ‡
        self.ç»¼åˆæ•°æ®['å®¢æˆ·é›†ä¸­åº¦'] = (1 / (self.ç»¼åˆæ•°æ®['å®¢æˆ·æ•°é‡'] + 1)).clip(0, 1)
        self.ç»¼åˆæ•°æ®['ä¾›åº”å•†é›†ä¸­åº¦'] = (1 / (self.ç»¼åˆæ•°æ®['ä¾›åº”å•†æ•°é‡'] + 1)).clip(0, 1)
        
        # å‘ç¥¨è´¨é‡æŒ‡æ ‡
        self.ç»¼åˆæ•°æ®['ç»¼åˆå‘ç¥¨è´¨é‡'] = (
            self.ç»¼åˆæ•°æ®['æœ‰æ•ˆå‘ç¥¨ç‡'] * 0.6 + 
            self.ç»¼åˆæ•°æ®['è¿›é¡¹æœ‰æ•ˆç‡'] * 0.4
        ).fillna(0.95).clip(0, 1)

    def _classify_industries_by_data(self):
        """åŸºäºçœŸå®æ•°æ®ç‰¹å¾è¿›è¡Œè¡Œä¸šåˆ†ç±»"""
        print("   - åŸºäºæ•°æ®ç‰¹å¾è¿›è¡Œè¡Œä¸šåˆ†ç±»...")
        
        # ç‰¹å¾å·¥ç¨‹ï¼šç”¨äºè¡Œä¸šèšç±»çš„ç‰¹å¾
        features_for_clustering = [
            'æ¯›åˆ©ç‡', 'æ”¶å…¥æˆæœ¬æ¯”', 'å®¢æˆ·é»æ€§', 
            'å®¢æˆ·é›†ä¸­åº¦', 'æœˆå‡å¢é•¿ç‡', 'èµ„äº§å‘¨è½¬ç‡ä»£ç†'
        ]
        
        # å‡†å¤‡èšç±»æ•°æ®ï¼Œæ·»åŠ æ•°æ®è´¨é‡æ£€æŸ¥
        cluster_data = self.ç»¼åˆæ•°æ®[features_for_clustering].copy()
        
        # æ•°æ®æ¸…ç†ï¼šå¤„ç†æ— ç©·å¤§å€¼å’Œå¼‚å¸¸å€¼
        print(f"   - èšç±»å‰æ•°æ®è´¨é‡æ£€æŸ¥...")
        print(f"     æ— ç©·å¤§å€¼æ•°é‡: {np.isinf(cluster_data.values).sum()}")
        print(f"     NaNå€¼æ•°é‡: {cluster_data.isnull().sum().sum()}")
        
        # æ›¿æ¢æ— ç©·å¤§å€¼ä¸ºNaNï¼Œç„¶åç”¨ä¸­ä½æ•°å¡«å……
        cluster_data = cluster_data.replace([np.inf, -np.inf], np.nan)
        
        # ç”¨ä¸­ä½æ•°å¡«å……NaNå€¼
        for col in cluster_data.columns:
            cluster_data[col] = cluster_data[col].fillna(cluster_data[col].median())
        
        # å¤„ç†æç«¯å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨3å€IQRè§„åˆ™ï¼‰
        for col in cluster_data.columns:
            Q1 = cluster_data[col].quantile(0.25)
            Q3 = cluster_data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            cluster_data[col] = cluster_data[col].clip(lower_bound, upper_bound)
        
        print(f"   - æ•°æ®æ¸…ç†å®Œæˆï¼Œæœ€ç»ˆç»Ÿè®¡:")
        print(f"     æ•°æ®å½¢çŠ¶: {cluster_data.shape}")
        print(f"     æ•°å€¼èŒƒå›´æ£€æŸ¥é€šè¿‡: {np.all(np.isfinite(cluster_data.values))}")
        
        try:
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            cluster_data_scaled = scaler.fit_transform(cluster_data)
            
            # K-meansèšç±»ï¼ˆ5ä¸ªè¡Œä¸šç±»åˆ«ï¼‰
            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(cluster_data_scaled)
            
            print(f"   - èšç±»æˆåŠŸï¼Œè¯†åˆ«å‡º {len(np.unique(clusters))} ä¸ªè¡Œä¸šç±»åˆ«")
            
        except Exception as e:
            print(f"   - èšç±»å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨åˆ†ç±»æ–¹æ¡ˆ")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºæ”¶å…¥è§„æ¨¡åˆ†ç±»
            clusters = pd.cut(
                self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'], 
                bins=5, 
                labels=range(5)
            ).fillna(0).astype(int)
            clusters = clusters.values
        
        # æ ¹æ®èšç±»ç»“æœå’Œç‰¹å¾åˆ†æå‘½åè¡Œä¸š
        try:
            cluster_centers = kmeans.cluster_centers_
        except:
            # å¤‡ç”¨ä¸­å¿ƒè®¡ç®—
            cluster_centers = []
            for i in range(5):
                mask = clusters == i
                if mask.sum() > 0:
                    center = cluster_data[mask].mean()
                else:
                    center = cluster_data.mean()
                cluster_centers.append(center)
        
        # åˆ†ææ¯ä¸ªèšç±»çš„ç‰¹å¾
        industry_mapping = {}
        industry_names = ['åˆ¶é€ ä¸š', 'æ‰¹å‘é›¶å”®', 'æœåŠ¡ä¸š', 'å»ºç­‘ä¸š', 'ç§‘æŠ€ä¼ä¸š']
        
        for i in range(5):
            cluster_mask = clusters == i
            if cluster_mask.sum() > 0:
                cluster_features = cluster_data[cluster_mask].mean()
                
                # åŸºäºç‰¹å¾ç‰¹ç‚¹åˆ†é…è¡Œä¸šåç§°
                if cluster_features['æ¯›åˆ©ç‡'] > 0.3 and cluster_features['å®¢æˆ·é›†ä¸­åº¦'] < 0.1:
                    industry_mapping[i] = 'ç§‘æŠ€ä¼ä¸š'
                elif cluster_features['æ”¶å…¥æˆæœ¬æ¯”'] < 1.2 and cluster_features['å®¢æˆ·é»æ€§'] > 1000000:
                    industry_mapping[i] = 'æ‰¹å‘é›¶å”®'
                elif cluster_features['æœˆå‡å¢é•¿ç‡'] > 0.05:
                    industry_mapping[i] = 'æœåŠ¡ä¸š'
                elif cluster_features['èµ„äº§å‘¨è½¬ç‡ä»£ç†'] > 500:
                    industry_mapping[i] = 'åˆ¶é€ ä¸š'
                else:
                    industry_mapping[i] = 'å»ºç­‘ä¸š'
            else:
                # å¦‚æœèšç±»ä¸ºç©ºï¼Œåˆ†é…é»˜è®¤è¡Œä¸š
                industry_mapping[i] = industry_names[i % len(industry_names)]
        
        # ç¡®ä¿æ‰€æœ‰è¡Œä¸šéƒ½æœ‰åˆ†é…
        used_industries = set(industry_mapping.values())
        unused_industries = set(industry_names) - used_industries
        
        for i, unused in enumerate(unused_industries):
            for cluster_id in range(5):
                if cluster_id not in industry_mapping:
                    industry_mapping[cluster_id] = unused
                    break
        
        self.ç»¼åˆæ•°æ®['è¡Œä¸šåˆ†ç±»'] = [industry_mapping[c] for c in clusters]
        
        # åŸºäºè¡Œä¸šåˆ†ç±»è®¡ç®—çœŸå®çš„è¡Œä¸šå‚æ•°
        self._calculate_industry_parameters()

    def _calculate_industry_parameters(self):
        """åŸºäºçœŸå®æ•°æ®è®¡ç®—è¡Œä¸šå‚æ•°"""
        print("   - è®¡ç®—çœŸå®è¡Œä¸šå‚æ•°...")
        
        self.è¡Œä¸šå‚æ•° = {}
        
        for industry in self.ç»¼åˆæ•°æ®['è¡Œä¸šåˆ†ç±»'].unique():
            industry_data = self.ç»¼åˆæ•°æ®[self.ç»¼åˆæ•°æ®['è¡Œä¸šåˆ†ç±»'] == industry]
            
            # è®¡ç®—è¡Œä¸šé£é™©ç³»æ•°ï¼ˆåŸºäºæ¯›åˆ©ç‡ç¨³å®šæ€§å’Œå®¢æˆ·é›†ä¸­åº¦ï¼‰
            risk_coeff = (
                1 - industry_data['æ¯›åˆ©ç‡'].mean() + 
                industry_data['å®¢æˆ·é›†ä¸­åº¦'].mean() + 
                abs(industry_data['æœˆå‡å¢é•¿ç‡'].mean()) * 2
            )
            risk_coeff = max(0.5, min(2.0, risk_coeff))
            
            # è®¡ç®—æµå¤±ç‡åŸºå‡†ï¼ˆåŸºäºå®¢æˆ·é»æ€§å’Œå¢é•¿ç‡ï¼‰
            loss_base = max(0.02, min(0.15, 
                0.1 - industry_data['å®¢æˆ·é»æ€§'].mean() / 10000000 + 
                abs(industry_data['æœˆå‡å¢é•¿ç‡'].mean())
            ))
            
            # è®¡ç®—æ•æ„Ÿæ€§ç³»æ•°ï¼ˆåŸºäºå‘ç¥¨è´¨é‡å’Œé›†ä¸­åº¦ï¼‰
            sensitivity = max(0.8, min(2.5,
                1 + (1 - industry_data['ç»¼åˆå‘ç¥¨è´¨é‡'].mean()) + 
                industry_data['å®¢æˆ·é›†ä¸­åº¦'].mean()
            ))
            
            self.è¡Œä¸šå‚æ•°[industry] = {
                'é£é™©ç³»æ•°': round(risk_coeff, 2),
                'æµå¤±ç‡åŸºå‡†': round(loss_base, 3),
                'æ•æ„Ÿæ€§ç³»æ•°': round(sensitivity, 2),
                'ä¼ä¸šæ•°é‡': len(industry_data)
            }
        
        print("   - è¡Œä¸šå‚æ•°è®¡ç®—å®Œæˆ:")
        for industry, params in self.è¡Œä¸šå‚æ•°.items():
            print(f"     {industry}: é£é™©ç³»æ•°={params['é£é™©ç³»æ•°']}, "
                  f"æµå¤±ç‡={params['æµå¤±ç‡åŸºå‡†']}, æ•æ„Ÿæ€§={params['æ•æ„Ÿæ€§ç³»æ•°']}, "
                  f"ä¼ä¸šæ•°={params['ä¼ä¸šæ•°é‡']}")

    def _enhance_data_quality(self):
        """æ•°æ®è´¨é‡å¢å¼º"""
        print("   - æ•°æ®è´¨é‡å¢å¼º...")
        
        # å¼‚å¸¸å€¼å¤„ç†ï¼šä½¿ç”¨99åˆ†ä½æ•°æˆªæ–­
        numeric_cols = ['å¹´æ”¶å…¥', 'å¹´æˆæœ¬', 'å®¢æˆ·æ•°é‡', 'ä¾›åº”å•†æ•°é‡', 'æ—¥å‡æ”¶å…¥']
        
        for col in numeric_cols:
            if col in self.ç»¼åˆæ•°æ®.columns:
                q99 = self.ç»¼åˆæ•°æ®[col].quantile(0.99)
                q1 = self.ç»¼åˆæ•°æ®[col].quantile(0.01)
                self.ç»¼åˆæ•°æ®[col] = self.ç»¼åˆæ•°æ®[col].clip(lower=q1, upper=q99)
        
        # ç¼ºå¤±å€¼æ™ºèƒ½å¡«å……
        self.ç»¼åˆæ•°æ®['æœˆå‡å¢é•¿ç‡'] = self.ç»¼åˆæ•°æ®['æœˆå‡å¢é•¿ç‡'].fillna(
            self.ç»¼åˆæ•°æ®.groupby('è¡Œä¸šåˆ†ç±»')['æœˆå‡å¢é•¿ç‡'].transform('median')
        )

    def calculate_ultimate_risk_scores(self):
        """ç»ˆæç‰ˆé£é™©è¯„åˆ†è®¡ç®—"""
        print("\nğŸ“Š è®¡ç®—ç»ˆæç‰ˆé£é™©è¯„åˆ†...")
        
        # å…­ç»´åº¦è¯„åˆ†ä½“ç³»ï¼ˆæ–°å¢ï¼šæˆé•¿æ€§å’Œæ•°æ®è´¨é‡ï¼‰
        self._calculate_financial_score_v3()
        self._calculate_business_stability_score_v3()
        self._calculate_operational_efficiency_score_v3()
        self._calculate_market_position_score_v3()
        self._calculate_growth_potential_score()
        self._calculate_data_quality_score()
        
        # åŠ¨æ€æƒé‡ï¼ˆåŸºäºRandomForestç‰¹å¾é‡è¦æ€§ï¼‰
        weights = self._calculate_dynamic_weights()
        
        # ç»¼åˆé£é™©è¯„åˆ†
        risk_components = [
            self.ç»¼åˆæ•°æ®['è´¢åŠ¡çŠ¶å†µè¯„åˆ†'],
            self.ç»¼åˆæ•°æ®['ä¸šåŠ¡ç¨³å®šæ€§è¯„åˆ†'],
            self.ç»¼åˆæ•°æ®['è¿è¥æ•ˆç‡è¯„åˆ†'],
            self.ç»¼åˆæ•°æ®['å¸‚åœºåœ°ä½è¯„åˆ†'],
            self.ç»¼åˆæ•°æ®['æˆé•¿æ½œåŠ›è¯„åˆ†'],
            self.ç»¼åˆæ•°æ®['æ•°æ®è´¨é‡è¯„åˆ†']
        ]
        
        self.ç»¼åˆæ•°æ®['ç»¼åˆé£é™©è¯„åˆ†'] = sum(w * score for w, score in zip(weights, risk_components))
        
        # ä¿®æ­£ï¼šLogisticè½¬æ¢ä¸ºè¿çº¦æ¦‚ç‡
        self._convert_risk_to_default_probability_v2()
        
        # ç«‹å³ä¿®æ­£ï¼šæœŸæœ›æŸå¤±ç‡è®¡ç®—
        self._calculate_expected_loss_corrected()
        
        print("âœ… ç»ˆæç‰ˆé£é™©è¯„åˆ†è®¡ç®—å®Œæˆ")

    def _calculate_financial_score_v3(self):
        """è´¢åŠ¡çŠ¶å†µè¯„åˆ†V3"""
        indicators = pd.DataFrame({
            'æ¯›åˆ©ç‡': self.ç»¼åˆæ•°æ®['æ¯›åˆ©ç‡'],
            'æ”¶å…¥æˆæœ¬æ¯”': self.ç»¼åˆæ•°æ®['æ”¶å…¥æˆæœ¬æ¯”'],
            'æµåŠ¨æ¯”ç‡ä»£ç†': self.ç»¼åˆæ•°æ®['æµåŠ¨æ¯”ç‡ä»£ç†'],
            'èµ„äº§å‘¨è½¬ç‡ä»£ç†': self.ç»¼åˆæ•°æ®['èµ„äº§å‘¨è½¬ç‡ä»£ç†']
        })
        
        scaler = StandardScaler()
        indicators_std = scaler.fit_transform(indicators.fillna(0))
        
        weights = [0.3, 0.25, 0.25, 0.2]
        scores = np.dot(indicators_std, weights)
        
        self.ç»¼åˆæ•°æ®['è´¢åŠ¡çŠ¶å†µè¯„åˆ†'] = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)

    def _calculate_business_stability_score_v3(self):
        """ä¸šåŠ¡ç¨³å®šæ€§è¯„åˆ†V3"""
        indicators = pd.DataFrame({
            'å®¢æˆ·åˆ†æ•£åº¦': 1 - self.ç»¼åˆæ•°æ®['å®¢æˆ·é›†ä¸­åº¦'],
            'ä¾›åº”å•†åˆ†æ•£åº¦': 1 - self.ç»¼åˆæ•°æ®['ä¾›åº”å•†é›†ä¸­åº¦'],
            'æ”¶å…¥ç¨³å®šæ€§': 1 / (abs(self.ç»¼åˆæ•°æ®['æœˆå‡å¢é•¿ç‡']) + 0.01),
            'è¥ä¸šè¿ç»­æ€§': np.log1p(self.ç»¼åˆæ•°æ®['è¥ä¸šå¤©æ•°'])
        })
        
        scaler = StandardScaler()
        indicators_std = scaler.fit_transform(indicators.fillna(0))
        
        weights = [0.3, 0.25, 0.25, 0.2]
        scores = np.dot(indicators_std, weights)
        
        self.ç»¼åˆæ•°æ®['ä¸šåŠ¡ç¨³å®šæ€§è¯„åˆ†'] = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)

    def _calculate_operational_efficiency_score_v3(self):
        """è¿è¥æ•ˆç‡è¯„åˆ†V3"""
        indicators = pd.DataFrame({
            'å®¢æˆ·æ•ˆç‡': self.ç»¼åˆæ•°æ®['å®¢æˆ·é»æ€§'],
            'å‘ç¥¨æ•ˆç‡': self.ç»¼åˆæ•°æ®['ç»¼åˆå‘ç¥¨è´¨é‡'],
            'æ—¥è¥æ”¶æ•ˆç‡': self.ç»¼åˆæ•°æ®['æ—¥å‡æ”¶å…¥'],
            'è§„æ¨¡æ•ˆåº”': np.log1p(self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'])
        })
        
        scaler = StandardScaler()
        indicators_std = scaler.fit_transform(indicators.fillna(0))
        
        weights = [0.3, 0.25, 0.25, 0.2]
        scores = np.dot(indicators_std, weights)
        
        self.ç»¼åˆæ•°æ®['è¿è¥æ•ˆç‡è¯„åˆ†'] = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)

    def _calculate_market_position_score_v3(self):
        """å¸‚åœºåœ°ä½è¯„åˆ†V3"""
        indicators = pd.DataFrame({
            'å¸‚åœºä»½é¢ä»£ç†': np.log1p(self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥']),
            'å®¢æˆ·åŸºç¡€': np.log1p(self.ç»¼åˆæ•°æ®['å®¢æˆ·æ•°é‡']),
            'ä¾›åº”é“¾ç½‘ç»œ': np.log1p(self.ç»¼åˆæ•°æ®['ä¾›åº”å•†æ•°é‡']),
            'äº¤æ˜“æ´»è·ƒåº¦': np.log1p(self.ç»¼åˆæ•°æ®['é”€é¡¹å‘ç¥¨æ•°'])
        })
        
        scaler = StandardScaler()
        indicators_std = scaler.fit_transform(indicators.fillna(0))
        
        weights = [0.4, 0.25, 0.2, 0.15]
        scores = np.dot(indicators_std, weights)
        
        self.ç»¼åˆæ•°æ®['å¸‚åœºåœ°ä½è¯„åˆ†'] = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)

    def _calculate_growth_potential_score(self):
        """æˆé•¿æ½œåŠ›è¯„åˆ†ï¼ˆæ–°å¢ï¼‰"""
        indicators = pd.DataFrame({
            'æ”¶å…¥å¢é•¿ç‡': self.ç»¼åˆæ•°æ®['æœˆå‡å¢é•¿ç‡'].clip(-1, 1),
            'æ¯›åˆ©ç‡æ°´å¹³': self.ç»¼åˆæ•°æ®['æ¯›åˆ©ç‡'].clip(-1, 1),
            'å¸‚åœºæ‰©å¼ èƒ½åŠ›': (self.ç»¼åˆæ•°æ®['å®¢æˆ·æ•°é‡'] / (self.ç»¼åˆæ•°æ®['è¥ä¸šå¤©æ•°'] + 1)).clip(0, 100),
            'è¿è¥å¤©æ•°': np.log1p(self.ç»¼åˆæ•°æ®['è¥ä¸šå¤©æ•°']).clip(0, 10)
        })
        
        # å¤„ç†æ— ç©·å¤§å€¼å’Œå¼‚å¸¸å€¼
        indicators = indicators.replace([np.inf, -np.inf], np.nan)
        indicators = indicators.fillna(indicators.median())
        
        # å†æ¬¡æ£€æŸ¥å’Œå¤„ç†æç«¯å€¼
        for col in indicators.columns:
            Q1 = indicators[col].quantile(0.25)
            Q3 = indicators[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            indicators[col] = indicators[col].clip(lower_bound, upper_bound)
        
        try:
            scaler = StandardScaler()
            indicators_std = scaler.fit_transform(indicators)
        except:
            # æ‰‹åŠ¨æ ‡å‡†åŒ–
            indicators_std = (indicators - indicators.mean()) / (indicators.std() + 1e-8)
            indicators_std = indicators_std.fillna(0).values
        
        weights = [0.4, 0.3, 0.2, 0.1]
        scores = np.dot(indicators_std, weights)
        
        self.ç»¼åˆæ•°æ®['æˆé•¿æ½œåŠ›è¯„åˆ†'] = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)

    def _calculate_data_quality_score(self):
        """æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆæ–°å¢ï¼‰"""
        indicators = pd.DataFrame({
            'å‘ç¥¨è´¨é‡': self.ç»¼åˆæ•°æ®['ç»¼åˆå‘ç¥¨è´¨é‡'].clip(0, 1),
            'æ•°æ®å®Œæ•´æ€§': (1 - (self.ç»¼åˆæ•°æ®[['å¹´æ”¶å…¥', 'å¹´æˆæœ¬', 'å®¢æˆ·æ•°é‡']].isnull().sum(axis=1) / 3)).clip(0, 1),
            'äº¤æ˜“é¢‘ç‡': np.log1p(self.ç»¼åˆæ•°æ®['é”€é¡¹å‘ç¥¨æ•°']).clip(0, 15),
            'ä¸šåŠ¡çœŸå®æ€§': (self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] / (self.ç»¼åˆæ•°æ®['é”€é¡¹å‘ç¥¨æ•°'] + 1)).clip(0, 1e6)
        })
        
        # å¤„ç†æ— ç©·å¤§å€¼å’Œå¼‚å¸¸å€¼
        indicators = indicators.replace([np.inf, -np.inf], np.nan)
        indicators = indicators.fillna(indicators.median())
        
        # å¤„ç†æç«¯å€¼
        for col in indicators.columns:
            Q1 = indicators[col].quantile(0.25)
            Q3 = indicators[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR
            upper_bound = Q3 + 3 * IQR
            indicators[col] = indicators[col].clip(lower_bound, upper_bound)
        
        try:
            scaler = StandardScaler()
            indicators_std = scaler.fit_transform(indicators)
        except:
            # æ‰‹åŠ¨æ ‡å‡†åŒ–
            indicators_std = (indicators - indicators.mean()) / (indicators.std() + 1e-8)
            indicators_std = indicators_std.fillna(0).values
        
        weights = [0.4, 0.3, 0.2, 0.1]
        scores = np.dot(indicators_std, weights)
        
        self.ç»¼åˆæ•°æ®['æ•°æ®è´¨é‡è¯„åˆ†'] = (scores - scores.min()) / (scores.max() - scores.min() + 1e-6)

    def _calculate_dynamic_weights(self):
        """åŸºäºéšæœºæ£®æ—è®¡ç®—åŠ¨æ€æƒé‡"""
        print("   - è®¡ç®—åŠ¨æ€æƒé‡ï¼ˆRandomForestç‰¹å¾é‡è¦æ€§ï¼‰...")
        
        # æ„é€ ç›®æ ‡å˜é‡ï¼ˆåŸºäºæ”¶å…¥è§„æ¨¡å’Œæ¯›åˆ©ç‡çš„ç»¼åˆæŒ‡æ ‡ï¼‰
        target = (
            0.6 * (self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'] / self.ç»¼åˆæ•°æ®['å¹´æ”¶å…¥'].max()) +
            0.4 * self.ç»¼åˆæ•°æ®['æ¯›åˆ©ç‡'].fillna(0)
        )
        
        # ç‰¹å¾çŸ©é˜µ
        features = self.ç»¼åˆæ•°æ®[[
            'è´¢åŠ¡çŠ¶å†µè¯„åˆ†', 'ä¸šåŠ¡ç¨³å®šæ€§è¯„åˆ†', 'è¿è¥æ•ˆç‡è¯„åˆ†',
            'å¸‚åœºåœ°ä½è¯„åˆ†', 'æˆé•¿æ½œåŠ›è¯„åˆ†', 'æ•°æ®è´¨é‡è¯„åˆ†'
        ]].fillna(0)
        
        # éšæœºæ£®æ—
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(features, target)
        
        # ç‰¹å¾é‡è¦æ€§ä½œä¸ºæƒé‡
        importances = rf.feature_importances_
        
        # å½’ä¸€åŒ–æƒé‡
        weights = importances / importances.sum()
        
        print(f"   - åŠ¨æ€æƒé‡: {weights.round(3)}")
        return weights

    def _convert_risk_to_default_probability_v2(self):
        """Logisticè½¬æ¢V2ï¼ˆä¿®æ­£å‚æ•°ï¼‰"""
        print("   - Logisticè¿çº¦æ¦‚ç‡è½¬æ¢...")
        
        # æ›´ç²¾ç¡®çš„å‚æ•°æ ‡å®šï¼šåŸºäºé“¶è¡Œå®é™…ç»éªŒ
        # é«˜é£é™©è¯„åˆ†(0.9) -> ä½è¿çº¦æ¦‚ç‡(2%)
        # ä½é£é™©è¯„åˆ†(0.1) -> é«˜è¿çº¦æ¦‚ç‡(30%)
        
        from scipy.optimize import fsolve
        
        def equations(params):
            alpha, beta = params
            eq1 = 1 / (1 + np.exp(alpha + beta * 0.9)) - 0.02  # é«˜åˆ†ä½è¿çº¦
            eq2 = 1 / (1 + np.exp(alpha + beta * 0.1)) - 0.30  # ä½åˆ†é«˜è¿çº¦
            return [eq1, eq2]
        
        try:
            alpha, beta = fsolve(equations, [3, -8])
            print(f"   - ä¿®æ­£Logisticå‚æ•°: Î±={alpha:.3f}, Î²={beta:.3f}")
        except:
            # å¤‡ç”¨å‚æ•°
            alpha, beta = 2.944, -7.313
            print(f"   - ä½¿ç”¨å¤‡ç”¨Logisticå‚æ•°: Î±={alpha:.3f}, Î²={beta:.3f}")
        
        # è®¡ç®—è¿çº¦æ¦‚ç‡
        risk_scores = self.ç»¼åˆæ•°æ®['ç»¼åˆé£é™©è¯„åˆ†']
        self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] = 1 / (1 + np.exp(alpha + beta * risk_scores))
        
        # ç¡®ä¿è¿çº¦æ¦‚ç‡åœ¨åˆç†èŒƒå›´å†…
        self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] = self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'].clip(0.01, 0.35)

    def _calculate_expected_loss_corrected(self):
        """ä¿®æ­£ï¼šæœŸæœ›æŸå¤±ç‡è®¡ç®—"""
        print("   - ä¿®æ­£æœŸæœ›æŸå¤±ç‡è®¡ç®—...")
        
        # ç«‹å³ä¿®æ­£ï¼šä½¿ç”¨è¿çº¦æ¦‚ç‡Piè€Œä¸æ˜¯Li
        self.ç»¼åˆæ•°æ®['æœŸæœ›æŸå¤±ç‡'] = self.ç»¼åˆæ•°æ®.apply(
            lambda row: (
                row['è¿çº¦æ¦‚ç‡'] *  # ä½¿ç”¨Piè€Œä¸æ˜¯Li
                self.è¡Œä¸šå‚æ•°[row['è¡Œä¸šåˆ†ç±»']]['æ•æ„Ÿæ€§ç³»æ•°'] * 
                0.6  # LGDè¿çº¦æŸå¤±ç‡
            ), axis=1
        )
        
        # é£é™©ç­‰çº§åˆ†ç±»ï¼ˆåŸºäºè¿çº¦æ¦‚ç‡ï¼‰
        self._classify_risk_levels_corrected()

    def _classify_risk_levels_corrected(self):
        """ä¿®æ­£é£é™©ç­‰çº§åˆ†ç±»"""
        conditions = [
            self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] <= 0.03,
            (self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] > 0.03) & (self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] <= 0.08),
            (self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] > 0.08) & (self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] <= 0.15),
            (self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] > 0.15) & (self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] <= 0.25),
            self.ç»¼åˆæ•°æ®['è¿çº¦æ¦‚ç‡'] > 0.25
        ]
        
        choices = ['ä¼˜è´¨', 'è‰¯å¥½', 'ä¸€èˆ¬', 'å…³æ³¨', 'æ¬¡çº§']
        self.ç»¼åˆæ•°æ®['é£é™©ç­‰çº§'] = np.select(conditions, choices, default='æ¬¡çº§')

    def design_ultimate_credit_strategy(self, total_budget=10000):
        """ç»ˆæç‰ˆä¿¡è´·ç­–ç•¥è®¾è®¡"""
        print("\nğŸ’° è®¾è®¡ç»ˆæç‰ˆä¿¡è´·ç­–ç•¥...")
        
        # 1. åŸºäºé™„ä»¶3çš„åŠ¨æ€åˆ©ç‡å®šä»·
        self._calculate_market_based_interest_rates()
        
        # 2. ç²¾ç¡®é¢åº¦è®¡ç®—
        self._calculate_precise_credit_limits()
        
        # 3. ä¿®æ­£ç›®æ ‡å‡½æ•°ä¼˜åŒ–
        self._optimize_expected_return(total_budget)
        
        print("âœ… ç»ˆæç‰ˆä¿¡è´·ç­–ç•¥è®¾è®¡å®Œæˆ")

    def _calculate_market_based_interest_rates(self):
        """åŸºäºé™„ä»¶3çš„å¸‚åœºåŒ–åˆ©ç‡å®šä»·"""
        print("   - åŸºäºå¸‚åœºæ•°æ®çš„åŠ¨æ€åˆ©ç‡å®šä»·...")
        
        # åŸºå‡†åˆ©ç‡ï¼ˆæ— é£é™©åˆ©ç‡ï¼‰
        base_rate = 0.04
        
        # æ ¹æ®é£é™©ç­‰çº§æ˜ å°„åˆ°é™„ä»¶3çš„ä¿¡èª‰ç­‰çº§
        credit_grade_mapping = {
            'ä¼˜è´¨': 'A',
            'è‰¯å¥½': 'A', 
            'ä¸€èˆ¬': 'B',
            'å…³æ³¨': 'B',
            'æ¬¡çº§': 'C'
        }
        
        rates = []
        for _, row in self.ç»¼åˆæ•°æ®.iterrows():
            risk_grade = row['é£é™©ç­‰çº§']
            credit_grade = credit_grade_mapping.get(risk_grade, 'C')
            
            # åŸºäºæœŸæœ›æŸå¤±ç‡ç¡®å®šåˆ©ç‡æ°´å¹³
            expected_loss = row['æœŸæœ›æŸå¤±ç‡']
            
            # ä½¿ç”¨æµå¤±ç‡æ¨¡å‹ç¡®å®šæœ€ä¼˜åˆ©ç‡
            if credit_grade in self.æµå¤±ç‡æ¨¡å‹:
                # ç›®æ ‡ï¼šæœ€å¤§åŒ– (åˆ©ç‡ - æœŸæœ›æŸå¤±ç‡) * (1 - æµå¤±ç‡)
                def profit_function(r):
                    if len(self.æµå¤±ç‡æ¨¡å‹[credit_grade]) == 3:
                        # æŒ‡æ•°æ¨¡å‹
                        a, b, c = self.æµå¤±ç‡æ¨¡å‹[credit_grade]
                        loss_rate = a * np.exp(b * r) + c
                    else:
                        # çº¿æ€§æ¨¡å‹
                        k, b = self.æµå¤±ç‡æ¨¡å‹[credit_grade]
                        loss_rate = k * r + b
                    
                    loss_rate = max(0, min(0.8, loss_rate))  # é™åˆ¶åœ¨åˆç†èŒƒå›´
                    return -(r - expected_loss) * (1 - loss_rate)
                
                # å¯»æ‰¾æœ€ä¼˜åˆ©ç‡
                from scipy.optimize import minimize_scalar
                result = minimize_scalar(profit_function, bounds=(0.04, 0.15), method='bounded')
                optimal_rate = result.x
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šåŸºäºæœŸæœ›æŸå¤±ç‡çš„ç®€å•å®šä»·
                optimal_rate = base_rate + expected_loss * 3 + 0.02
            
            rates.append(max(0.04, min(0.15, optimal_rate)))
        
        self.ç»¼åˆæ•°æ®['æ¨èåˆ©ç‡'] = rates

    def _calculate_precise_credit_limits(self):
        """ç²¾ç¡®é¢åº¦è®¡ç®—"""
        print("   - ç²¾ç¡®ä¿¡è´·é¢åº¦è®¡ç®—...")
        
        # å¤šå› å­é¢åº¦æ¨¡å‹
        limits = []
        for _, row in self.ç»¼åˆæ•°æ®.iterrows():
            # æ”¶å…¥åŸºç¡€é¢åº¦
            income_base = np.log1p(row['å¹´æ”¶å…¥']) / 15
            
            # é£é™©è°ƒæ•´ï¼ˆä½¿ç”¨æœŸæœ›æŸå¤±ç‡ï¼‰
            risk_adj = (1 - row['æœŸæœ›æŸå¤±ç‡']) ** 1.5
            
            # è¡Œä¸šè°ƒæ•´
            industry_adj = 1 / self.è¡Œä¸šå‚æ•°[row['è¡Œä¸šåˆ†ç±»']]['é£é™©ç³»æ•°']
            
            # æˆé•¿æ€§è°ƒæ•´
            growth_adj = 1 + max(-0.2, min(0.3, row['æœˆå‡å¢é•¿ç‡']))
            
            # æ•°æ®è´¨é‡è°ƒæ•´
            quality_adj = row['æ•°æ®è´¨é‡è¯„åˆ†']
            
            # ç»¼åˆé¢åº¦
            credit_limit = income_base * risk_adj * industry_adj * growth_adj * quality_adj * 80
            
            # é£é™©é—¨æ§›
            if row['è¿çº¦æ¦‚ç‡'] > 0.25:
                credit_limit = 0
            elif row['é£é™©ç­‰çº§'] == 'æ¬¡çº§':
                credit_limit = min(credit_limit, 50)
            
            limits.append(max(0, min(500, credit_limit)))
        
        self.ç»¼åˆæ•°æ®['æ¨èé¢åº¦'] = limits

    def _optimize_expected_return(self, total_budget):
        """ä¿®æ­£ï¼šæœŸæœ›æ”¶ç›Šä¼˜åŒ–"""
        print("   - æœŸæœ›æ”¶ç›Šä¼˜åŒ–...")
        
        # ä¿®æ­£ç›®æ ‡å‡½æ•°ï¼šæœŸæœ›æ”¶ç›Š = (åˆ©ç‡ - æœŸæœ›æŸå¤±ç‡) Ã— é¢åº¦
        self.ç»¼åˆæ•°æ®['æœŸæœ›æ”¶ç›Šç‡'] = self.ç»¼åˆæ•°æ®['æ¨èåˆ©ç‡'] - self.ç»¼åˆæ•°æ®['æœŸæœ›æŸå¤±ç‡']
        
        # å•ä½èµ„æœ¬æœŸæœ›æ”¶ç›Š
        self.ç»¼åˆæ•°æ®['å•ä½èµ„æœ¬æ”¶ç›Š'] = (
            self.ç»¼åˆæ•°æ®['æœŸæœ›æ”¶ç›Šç‡'] * self.ç»¼åˆæ•°æ®['ç»¼åˆé£é™©è¯„åˆ†']
        )
        
        # ä¼˜åŒ–åˆ†é…
        eligible = self.ç»¼åˆæ•°æ®[self.ç»¼åˆæ•°æ®['æ¨èé¢åº¦'] > 0].copy()
        eligible = eligible.sort_values('å•ä½èµ„æœ¬æ”¶ç›Š', ascending=False)
        
        allocated_budget = 0
        final_amounts = []
        
        for idx, row in eligible.iterrows():
            if allocated_budget + row['æ¨èé¢åº¦'] <= total_budget:
                final_amounts.append(row['æ¨èé¢åº¦'])
                allocated_budget += row['æ¨èé¢åº¦']
            elif total_budget - allocated_budget >= 10:
                final_amounts.append(total_budget - allocated_budget)
                allocated_budget = total_budget
                break
            else:
                final_amounts.append(0)
        
        # è¡¥é½å‰©ä½™
        while len(final_amounts) < len(eligible):
            final_amounts.append(0)
        
        eligible['æœ€ç»ˆé¢åº¦'] = final_amounts
        
        # æ›´æ–°ä¸»æ•°æ®
        self.ç»¼åˆæ•°æ®['æœ€ç»ˆé¢åº¦'] = 0
        self.ç»¼åˆæ•°æ®.loc[eligible.index, 'æœ€ç»ˆé¢åº¦'] = eligible['æœ€ç»ˆé¢åº¦']
        
        self.ç»¼åˆæ•°æ®['è´·æ¬¾å†³ç­–'] = self.ç»¼åˆæ•°æ®['æœ€ç»ˆé¢åº¦'].apply(
            lambda x: 'æ‰¹å‡†' if x > 0 else 'æ‹’ç»'
        )

    def generate_ultimate_report(self):
        """ç”Ÿæˆç»ˆæç‰ˆåˆ†ææŠ¥å‘Š"""
        print("\nğŸ“Š ç”Ÿæˆç»ˆæç‰ˆåˆ†ææŠ¥å‘Š...")
        
        approved = self.ç»¼åˆæ•°æ®[self.ç»¼åˆæ•°æ®['æœ€ç»ˆé¢åº¦'] > 0]
        
        print("\n=== ç»ˆæç‰ˆä¿¡è´·åˆ†æç»“æœ ===")
        print(f"æ€»ç”³è¯·ä¼ä¸š: {len(self.ç»¼åˆæ•°æ®)}å®¶")
        print(f"æ‰¹å‡†ä¼ä¸š: {len(approved)}å®¶")
        print(f"æ‰¹å‡†ç‡: {len(approved)/len(self.ç»¼åˆæ•°æ®)*100:.1f}%")
        
        if len(approved) > 0:
            print(f"æ€»æ”¾è´·é‡‘é¢: {approved['æœ€ç»ˆé¢åº¦'].sum():.0f}ä¸‡å…ƒ")
            print(f"å¹³å‡é¢åº¦: {approved['æœ€ç»ˆé¢åº¦'].mean():.1f}ä¸‡å…ƒ")
            print(f"å¹³å‡åˆ©ç‡: {approved['æ¨èåˆ©ç‡'].mean()*100:.2f}%")
            print(f"å¹³å‡è¿çº¦æ¦‚ç‡: {approved['è¿çº¦æ¦‚ç‡'].mean()*100:.2f}%")
            print(f"å¹³å‡æœŸæœ›æŸå¤±ç‡: {approved['æœŸæœ›æŸå¤±ç‡'].mean()*100:.2f}%")
            
            # æœŸæœ›æ”¶ç›Š
            total_expected_return = (
                approved['æœ€ç»ˆé¢åº¦'] * approved['æœŸæœ›æ”¶ç›Šç‡']
            ).sum()
            print(f"é¢„æœŸå¹´æ”¶ç›Š: {total_expected_return:.0f}ä¸‡å…ƒ")
            
            # ROEä¼°ç®—
            roe = total_expected_return / approved['æœ€ç»ˆé¢åº¦'].sum() * 100
            print(f"èµ„æœ¬æ”¶ç›Šç‡(ROE): {roe:.2f}%")
        
        # é£é™©åˆ†å¸ƒ
        print("\næŒ‰é£é™©ç­‰çº§åˆ†å¸ƒ:")
        risk_dist = self.ç»¼åˆæ•°æ®['é£é™©ç­‰çº§'].value_counts()
        for level, count in risk_dist.items():
            percentage = count / len(self.ç»¼åˆæ•°æ®) * 100
            approved_count = len(approved[approved['é£é™©ç­‰çº§'] == level])
            approval_rate = approved_count / count * 100 if count > 0 else 0
            print(f"   {level}: {count}å®¶ ({percentage:.1f}%), è·è´·: {approved_count}å®¶ ({approval_rate:.1f}%)")
        
        # è¡Œä¸šåˆ†å¸ƒ
        print("\næŒ‰è¡Œä¸šåˆ†å¸ƒ:")
        for industry in self.ç»¼åˆæ•°æ®['è¡Œä¸šåˆ†ç±»'].unique():
            industry_data = self.ç»¼åˆæ•°æ®[self.ç»¼åˆæ•°æ®['è¡Œä¸šåˆ†ç±»'] == industry]
            industry_approved = approved[approved['è¡Œä¸šåˆ†ç±»'] == industry]
            
            print(f"   {industry}: {len(industry_data)}å®¶, è·è´·: {len(industry_approved)}å®¶ "
                  f"({len(industry_approved)/len(industry_data)*100:.1f}%)")
        
        # å¯¼å‡ºç»“æœ
        self._export_ultimate_results()

    def _export_ultimate_results(self):
        """å¯¼å‡ºç»ˆæç‰ˆç»“æœ"""
        output_cols = [
            'ä¼ä¸šä»£å·', 'ä¼ä¸šåç§°', 'è¡Œä¸šåˆ†ç±»', 'é£é™©ç­‰çº§', 
            'ç»¼åˆé£é™©è¯„åˆ†', 'è¿çº¦æ¦‚ç‡', 'æœŸæœ›æŸå¤±ç‡',
            'å¹´æ”¶å…¥', 'å¹´æˆæœ¬', 'æ¯›åˆ©ç‡', 'æœˆå‡å¢é•¿ç‡',
            'å®¢æˆ·æ•°é‡', 'ä¾›åº”å•†æ•°é‡', 'è¥ä¸šå¤©æ•°',
            'è´¢åŠ¡çŠ¶å†µè¯„åˆ†', 'ä¸šåŠ¡ç¨³å®šæ€§è¯„åˆ†', 'è¿è¥æ•ˆç‡è¯„åˆ†', 
            'å¸‚åœºåœ°ä½è¯„åˆ†', 'æˆé•¿æ½œåŠ›è¯„åˆ†', 'æ•°æ®è´¨é‡è¯„åˆ†',
            'æ¨èé¢åº¦', 'æœ€ç»ˆé¢åº¦', 'æ¨èåˆ©ç‡', 'æœŸæœ›æ”¶ç›Šç‡', 'è´·æ¬¾å†³ç­–'
        ]
        
        for col in output_cols:
            if col not in self.ç»¼åˆæ•°æ®.columns:
                self.ç»¼åˆæ•°æ®[col] = 0
        
        result_file = 'problem2_ultimate_analysis_results.xlsx'
        self.ç»¼åˆæ•°æ®[output_cols].to_excel(result_file, index=False)
        print(f"\nâœ… ç»ˆæç‰ˆåˆ†æç»“æœå·²å¯¼å‡ºè‡³: {result_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("é—®é¢˜2ç»ˆæä¼˜åŒ–ç‰ˆï¼šåŸºäºé™„ä»¶2&3çš„æ·±åº¦æ”¹è¿›")
    print("ç«‹å³ä¿®æ­£ï¼šLiâ†’Pi, æœŸæœ›æ”¶ç›Šä¼˜åŒ–")
    print("ä¼˜å…ˆè¡¥å……ï¼šçœŸå®è¡Œä¸šå‚æ•°, å¸‚åœºåŒ–åˆ©ç‡å®šä»·")
    print("="*80)
    
    analyzer = UltimateEnterpriseAnalyzer()
    
    # åŠ è½½æ•°æ®
    if not analyzer.load_data():
        return
    
    # æ„å»ºåˆ©ç‡-æµå¤±ç‡æ¨¡å‹
    analyzer.build_interest_loss_model()
    
    # æ•°æ®é¢„å¤„ç†
    analyzer.preprocess_data_ultimate()
    
    # è®¡ç®—é£é™©è¯„åˆ†
    analyzer.calculate_ultimate_risk_scores()
    
    # åˆ¶å®šä¿¡è´·ç­–ç•¥
    analyzer.design_ultimate_credit_strategy(total_budget=10000)
    
    # ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_ultimate_report()
    
    print("\nğŸ‰ é—®é¢˜2ç»ˆæä¼˜åŒ–ç‰ˆåˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
